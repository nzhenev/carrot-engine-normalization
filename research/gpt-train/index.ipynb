{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Carrot GPT2 Normalization\n",
        "Train chat GPT2 Model to normalize data."
      ],
      "metadata": {
        "id": "X5R6T-Nt0f1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update\n",
        "# !apt-get install -y nvidia-driver-470\n",
        "# !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
      ],
      "metadata": {
        "id": "sOXKjFqlU0h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install --upgrade transformers accelerate\n",
        "!pip install werpy"
      ],
      "metadata": {
        "id": "PrKF9BbMCGaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import werpy\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "id": "U2kzwS3zRT9X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hk0ihunDBvmR"
      },
      "outputs": [],
      "source": [
        "dataRAW = load_dataset(\"sellersew/carrot-engine-normalization-translation-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataRAW[\"train\"].train_test_split(test_size=0.005, train_size=0.005, shuffle=True)"
      ],
      "metadata": {
        "id": "pjgLhswgB3yr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFVMr0HwCSFR",
        "outputId": "028c8479-d15f-4fb9-d359-a3440f633d32"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input', 'output'],\n",
              "        num_rows: 15285\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input', 'output'],\n",
              "        num_rows: 15286\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "prefix    = \"normalize the following text from its written form into its verbalized form: \"\n",
        "\n",
        "def preprocess_function(dataset):\n",
        "    inputs       = [prefix + entry for entry in dataset[\"input\"]]\n",
        "    targets      = [entry for entry in dataset[\"output\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "OUnD1D-oCkYz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = data.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "rD3lS84wCzLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model         = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "fEwf9vYFHFMs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ZLCdBF4APmDP",
        "outputId": "a6bcab9f-de6c-4f0e-b2a0-06e5be35a5fd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9560' max='9560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9560/9560 20:11, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.976300</td>\n",
              "      <td>0.293482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.348300</td>\n",
              "      <td>0.193091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.258400</td>\n",
              "      <td>0.146480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.212500</td>\n",
              "      <td>0.122463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.180600</td>\n",
              "      <td>0.108390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.099465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>0.094226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.137800</td>\n",
              "      <td>0.091075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.138300</td>\n",
              "      <td>0.089508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.088815</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9560, training_loss=0.24144028110982982, metrics={'train_runtime': 1211.7847, 'train_samples_per_second': 126.136, 'train_steps_per_second': 7.889, 'total_flos': 1710252489768960.0, 'train_loss': 0.24144028110982982, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./normalize-model-10-epoch\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./normalize-model-10-epoch\")"
      ],
      "metadata": {
        "id": "9GiTQ2N9-aqB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark\n",
        "We will now benchmark the dataset on our test dataset."
      ],
      "metadata": {
        "id": "rPHx-FY7QPWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data = load_dataset(\"sellersew/carrot-engine-normalization-translation-v2\", data_files=\"native-tests.csv\")"
      ],
      "metadata": {
        "id": "bz6vll-TRFji"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalulate(text):\n",
        "    prompt  = prefix + text\n",
        "    inputs  = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    inputs  = inputs.to(model.device)\n",
        "    outputs = model.generate(inputs, max_length=128)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "ShafphiHPulM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "start_time = time.time()\n",
        "passing    = 0\n",
        "failing    = 0\n",
        "WER        = []\n",
        "\n",
        "for entry in tqdm(testing_data[\"train\"], desc='Processing', unit='iteration'):\n",
        "    input  = entry[\"input\"]\n",
        "    output = evalulate(input)\n",
        "    WER.append(werpy.wer(input, output))\n",
        "    if input == output:\n",
        "        passing += 1\n",
        "    else:\n",
        "        failing += 1\n",
        "\n",
        "time_taken = time.time() - start_time\n",
        "average    = sum(WER) / len(WER)\n",
        "print(\"Passing: \" + str(passing))\n",
        "print(\"Failing: \" + str(failing))\n",
        "print(\"WER    : \" + str(average))\n",
        "print(f'Time : {time_taken:.2f} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejg6XyBS2OwG",
        "outputId": "c7cc0280-738c-4110-c697-f2799d29a816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:  37%|███▋      | 335/906 [04:04<06:41,  1.42iteration/s]"
          ]
        }
      ]
    }
  ]
}